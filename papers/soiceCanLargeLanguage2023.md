# Can large language models democratize access to dual-use biotechnology?

**tl;dr** yes

Can LLMs take general knowledge and weaponize it? In this paper, they specifically deal with the issue of how it is becoming increasingly accessible to synthesize pandemic-causing viruses *without* any specific domain knowledge. The specific methodology is asking people who don't have doctorate training in the sciences to spend an hour prompting LLMs and see how much information on synthesizing dangerous viruses they can get.

With jailbreaking prompts, it's extremely easy to get LLMs to give you dangerous information if you're able to frame your response right. In this way, why don't you set up some sort of trust system? Is it dystopian to have some sort of unique identifier? How could you prevent this from being jail-broken? If it is just the same LLM evaluating trust, then the same jail-breaking methods would break it.

The paper recommends DNA based nonproliferation or just taking out dangerous papers from an LLM training set. If anything, I think the DNA nonproliferation is only viable method. To try and take out dangerous training materials means that we know everything that is dangerous or at least enough to prevent catastrophe. I don't see why these dual-use technologies are limited to things like nuclear and biotechnology and why the same concept couldn't be extended to... *every technology*?